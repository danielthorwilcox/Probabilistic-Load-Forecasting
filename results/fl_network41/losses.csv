,epoch,training loss,validation loss
0,1.0,0.872730419780668,0.6198332766433815
1,2.0,0.48536956590183533,0.4187681024724787
2,3.0,0.35783532788740335,0.3512760756851791
3,4.0,0.306763698300604,0.32154689555044297
4,5.0,0.286049621457553,0.3107958569542154
5,6.0,0.27221989763375803,0.30531266660659345
6,7.0,0.26431072194602606,0.3076776610566424
7,8.0,0.2548857946257565,0.30979944262411685
8,9.0,0.24828210999952496,0.31099426107747213
9,10.0,0.24330690264372537,0.3048128369566682
10,11.0,0.23569310106625213,0.3037793719536298
11,12.0,0.23820626200562683,0.318376603064599
12,13.0,0.2273547898833923,0.32305139419320344
13,14.0,0.21988070867338233,0.32963080820325136
14,15.0,0.21166262236418645,0.3248175446476255
15,16.0,0.22013226390214255,0.5263821730366001
16,17.0,0.27909267401497667,0.3944146025490451
17,18.0,0.2414340293703817,0.3751510137861425
18,19.0,0.23133301446780316,0.37031364479622286
19,20.0,0.22610971183408032,0.3679765633174351
20,21.0,0.22178376815924988,0.3641200626825357
21,22.0,0.2188993120226412,0.3636281567734557
22,23.0,0.21593762588435114,0.3658250442573002
23,24.0,0.21343952697284974,0.3562142032307464
24,25.0,0.21219315136993788,0.3579596888709378
